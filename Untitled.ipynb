{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out:\n",
      " torch.Size([3, 2, 4]) \n",
      " tensor([[[-0.1274, -0.4900, -0.0031,  0.3730],\n",
      "         [-0.0414, -0.0342, -0.0346,  0.4281]],\n",
      "\n",
      "        [[ 0.2446, -0.4037,  0.0968,  0.2307],\n",
      "         [ 0.0374, -0.2496,  0.1120,  0.1745]],\n",
      "\n",
      "        [[ 0.3370,  0.2009,  0.5273,  0.1411],\n",
      "         [ 0.3217, -0.2838,  0.2881,  0.0010]]])\n",
      "hn:\n",
      " torch.Size([1, 3, 4]) \n",
      " tensor([[[-0.0414, -0.0342, -0.0346,  0.4281],\n",
      "         [ 0.0374, -0.2496,  0.1120,  0.1745],\n",
      "         [ 0.3217, -0.2838,  0.2881,  0.0010]]])\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "from torch import nn\n",
    "from torch.autograd import Variable as V\n",
    " \n",
    "layer = 1\n",
    " \n",
    "t.manual_seed(1000)\n",
    "# 3句话，每句话2个字，每个字4维矢量\n",
    "# batch为3，step为2，每个元素4维\n",
    "input = V(t.randn(3,2,4))\n",
    "# 1层，输出(隐藏)神经元3维，输入神经元4维\n",
    "# 1层，3隐藏神经元，每个元素4维\n",
    "lstm = nn.LSTM(4,4,layer,batch_first=True)\n",
    "# 初始状态：1层，batch为3,隐藏神经元3\n",
    "h0 = V(t.randn(layer,3,4))\n",
    "c0 = V(t.randn(layer,3,4))\n",
    " \n",
    "out, hn = lstm(input,(h0,c0))\n",
    "print(\"out:\\n\",out.shape ,\"\\n\", out)\n",
    "print(\"hn:\\n\",hn[0].size(),\"\\n\",hn[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0414, -0.0342, -0.0346,  0.4281],\n",
      "        [ 0.0374, -0.2496,  0.1120,  0.1745],\n",
      "        [ 0.3217, -0.2838,  0.2881,  0.0010]])\n"
     ]
    }
   ],
   "source": [
    "print(out[:,-1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1274, -0.4900, -0.0031,  0.3730],\n",
      "         [-0.0414, -0.0342, -0.0346,  0.4281]],\n",
      "\n",
      "        [[ 0.2446, -0.4037,  0.0968,  0.2307],\n",
      "         [ 0.0374, -0.2496,  0.1120,  0.1745]],\n",
      "\n",
      "        [[ 0.3370,  0.2009,  0.5273,  0.1411],\n",
      "         [ 0.3217, -0.2838,  0.2881,  0.0010]]])\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.autograd import  Variable\n",
    "import torch.nn as nn\n",
    "x = np.random.randn(1000, 1)*4\n",
    "w = np.array([0.5,])\n",
    "bias = -1.68\n",
    "\n",
    "y_true = np.dot(x, w) + bias  #真实数据\n",
    "y = y_true + np.random.randn(x.shape[0])#加噪声的数据\n",
    "#我们需要使用x和y，以及y_true回归出w和bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRression(nn.Module):\n",
    "    def __init__(self, input_size, out_size):\n",
    "        super(LinearRression, self).__init__()\n",
    "        self.x2o = nn.Linear(input_size, out_size)\n",
    "    #初始化\n",
    "    def forward(self, x):\n",
    "        return self.x2o(x)\n",
    "    #前向传递"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:1\" if gpu else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "model = LinearRression(1, 1)#回归模型\n",
    "criterion = nn.MSELoss()  #损失函数\n",
    "#调用cuda\n",
    "#model.cuda()\n",
    "#criterion.cuda()\n",
    "model.to(device)\n",
    "criterion.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/R2016hwang/.pyenv/versions/3.6.2/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/home/R2016hwang/.pyenv/versions/3.6.2/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch[9]: 7.164\n",
      "Loss at epoch[19]: 6.390\n",
      "Loss at epoch[29]: 4.123\n",
      "Loss at epoch[39]: 1.305\n",
      "Loss at epoch[49]: 1.300\n",
      "Loss at epoch[59]: 1.241\n",
      "Loss at epoch[69]: 0.700\n",
      "Loss at epoch[79]: 1.460\n",
      "Loss at epoch[89]: 0.745\n",
      "Loss at epoch[99]: 0.878\n",
      "Loss at epoch[109]: 1.620\n",
      "Loss at epoch[119]: 2.741\n",
      "Loss at epoch[129]: 1.493\n",
      "Loss at epoch[139]: 1.596\n",
      "Loss at epoch[149]: 1.420\n",
      "Loss at epoch[159]: 1.314\n",
      "Loss at epoch[169]: 0.233\n",
      "Loss at epoch[179]: 1.124\n",
      "Loss at epoch[189]: 0.713\n",
      "Loss at epoch[199]: 0.769\n",
      "Loss at epoch[209]: 0.797\n",
      "Loss at epoch[219]: 1.254\n",
      "Loss at epoch[229]: 0.601\n",
      "Loss at epoch[239]: 0.717\n",
      "Loss at epoch[249]: 1.489\n",
      "Loss at epoch[259]: 1.066\n",
      "Loss at epoch[269]: 1.098\n",
      "Loss at epoch[279]: 1.046\n",
      "Loss at epoch[289]: 0.420\n",
      "Loss at epoch[299]: 1.068\n",
      "Loss at epoch[309]: 1.637\n",
      "Loss at epoch[319]: 0.881\n",
      "Loss at epoch[329]: 0.997\n",
      "Loss at epoch[339]: 0.569\n",
      "Loss at epoch[349]: 0.968\n",
      "Loss at epoch[359]: 1.566\n",
      "Loss at epoch[369]: 0.841\n",
      "Loss at epoch[379]: 1.683\n",
      "Loss at epoch[389]: 1.477\n",
      "Loss at epoch[399]: 1.712\n",
      "Loss at epoch[409]: 0.425\n",
      "Loss at epoch[419]: 0.692\n",
      "Loss at epoch[429]: 1.343\n",
      "Loss at epoch[439]: 0.709\n",
      "Loss at epoch[449]: 1.272\n",
      "Loss at epoch[459]: 2.883\n",
      "Loss at epoch[469]: 1.321\n",
      "Loss at epoch[479]: 1.052\n",
      "Loss at epoch[489]: 0.540\n",
      "Loss at epoch[499]: 0.332\n",
      "Loss at epoch[509]: 1.288\n",
      "Loss at epoch[519]: 0.478\n",
      "Loss at epoch[529]: 0.559\n",
      "Loss at epoch[539]: 1.100\n",
      "Loss at epoch[549]: 1.588\n",
      "Loss at epoch[559]: 0.755\n",
      "Loss at epoch[569]: 0.731\n",
      "Loss at epoch[579]: 0.764\n",
      "Loss at epoch[589]: 0.837\n",
      "Loss at epoch[599]: 0.655\n",
      "Loss at epoch[609]: 1.865\n",
      "Loss at epoch[619]: 1.012\n",
      "Loss at epoch[629]: 0.637\n",
      "Loss at epoch[639]: 0.653\n",
      "Loss at epoch[649]: 0.487\n",
      "Loss at epoch[659]: 1.319\n",
      "Loss at epoch[669]: 0.835\n",
      "Loss at epoch[679]: 2.427\n",
      "Loss at epoch[689]: 1.190\n",
      "Loss at epoch[699]: 0.588\n",
      "Loss at epoch[709]: 0.643\n",
      "Loss at epoch[719]: 1.081\n",
      "Loss at epoch[729]: 0.603\n",
      "Loss at epoch[739]: 1.353\n",
      "Loss at epoch[749]: 1.235\n",
      "Loss at epoch[759]: 1.681\n",
      "Loss at epoch[769]: 1.171\n",
      "Loss at epoch[779]: 0.798\n",
      "Loss at epoch[789]: 0.862\n",
      "Loss at epoch[799]: 0.953\n",
      "Loss at epoch[809]: 0.700\n",
      "Loss at epoch[819]: 1.570\n",
      "Loss at epoch[829]: 0.828\n",
      "Loss at epoch[839]: 1.345\n",
      "Loss at epoch[849]: 0.712\n",
      "Loss at epoch[859]: 0.435\n",
      "Loss at epoch[869]: 0.859\n",
      "Loss at epoch[879]: 0.619\n",
      "Loss at epoch[889]: 0.597\n",
      "Loss at epoch[899]: 0.981\n",
      "Loss at epoch[909]: 1.057\n",
      "Loss at epoch[919]: 0.599\n",
      "Loss at epoch[929]: 1.772\n",
      "Loss at epoch[939]: 0.341\n",
      "Loss at epoch[949]: 0.918\n",
      "Loss at epoch[959]: 0.715\n",
      "Loss at epoch[969]: 1.502\n",
      "Loss at epoch[979]: 1.070\n",
      "Loss at epoch[989]: 0.877\n",
      "Loss at epoch[999]: 1.606\n",
      "Loss at epoch[1009]: 1.004\n",
      "Loss at epoch[1019]: 1.414\n",
      "Loss at epoch[1029]: 0.967\n",
      "Loss at epoch[1039]: 1.511\n",
      "Loss at epoch[1049]: 2.116\n",
      "Loss at epoch[1059]: 1.018\n",
      "Loss at epoch[1069]: 0.566\n",
      "Loss at epoch[1079]: 0.645\n",
      "Loss at epoch[1089]: 1.185\n",
      "Loss at epoch[1099]: 1.472\n",
      "Loss at epoch[1109]: 1.332\n",
      "Loss at epoch[1119]: 0.668\n",
      "Loss at epoch[1129]: 1.413\n",
      "Loss at epoch[1139]: 0.962\n",
      "Loss at epoch[1149]: 1.052\n",
      "Loss at epoch[1159]: 1.593\n",
      "Loss at epoch[1169]: 2.845\n",
      "Loss at epoch[1179]: 0.686\n",
      "Loss at epoch[1189]: 0.482\n",
      "Loss at epoch[1199]: 1.028\n",
      "Loss at epoch[1209]: 1.767\n",
      "Loss at epoch[1219]: 0.625\n",
      "Loss at epoch[1229]: 0.831\n",
      "Loss at epoch[1239]: 0.536\n",
      "Loss at epoch[1249]: 2.171\n",
      "Loss at epoch[1259]: 1.705\n",
      "Loss at epoch[1269]: 0.583\n",
      "Loss at epoch[1279]: 1.730\n",
      "Loss at epoch[1289]: 1.337\n",
      "Loss at epoch[1299]: 1.003\n",
      "Loss at epoch[1309]: 1.266\n",
      "Loss at epoch[1319]: 1.040\n",
      "Loss at epoch[1329]: 0.817\n",
      "Loss at epoch[1339]: 0.970\n",
      "Loss at epoch[1349]: 1.079\n",
      "Loss at epoch[1359]: 0.927\n",
      "Loss at epoch[1369]: 0.687\n",
      "Loss at epoch[1379]: 1.422\n",
      "Loss at epoch[1389]: 0.915\n",
      "Loss at epoch[1399]: 1.352\n",
      "Loss at epoch[1409]: 1.490\n",
      "Loss at epoch[1419]: 1.330\n",
      "Loss at epoch[1429]: 1.546\n",
      "Loss at epoch[1439]: 1.001\n",
      "Loss at epoch[1449]: 1.152\n",
      "Loss at epoch[1459]: 0.664\n",
      "Loss at epoch[1469]: 0.863\n",
      "Loss at epoch[1479]: 1.771\n",
      "Loss at epoch[1489]: 0.935\n",
      "Loss at epoch[1499]: 1.966\n",
      "Loss at epoch[1509]: 1.335\n",
      "Loss at epoch[1519]: 1.443\n",
      "Loss at epoch[1529]: 1.574\n",
      "Loss at epoch[1539]: 0.893\n",
      "Loss at epoch[1549]: 0.623\n",
      "Loss at epoch[1559]: 0.681\n",
      "Loss at epoch[1569]: 1.445\n",
      "Loss at epoch[1579]: 1.373\n",
      "Loss at epoch[1589]: 2.313\n",
      "Loss at epoch[1599]: 0.834\n",
      "Loss at epoch[1609]: 0.818\n",
      "Loss at epoch[1619]: 0.984\n",
      "Loss at epoch[1629]: 0.824\n",
      "Loss at epoch[1639]: 1.575\n",
      "Loss at epoch[1649]: 0.816\n",
      "Loss at epoch[1659]: 1.472\n",
      "Loss at epoch[1669]: 0.454\n",
      "Loss at epoch[1679]: 0.833\n",
      "Loss at epoch[1689]: 1.021\n",
      "Loss at epoch[1699]: 1.421\n",
      "Loss at epoch[1709]: 2.431\n",
      "Loss at epoch[1719]: 0.528\n",
      "Loss at epoch[1729]: 1.436\n",
      "Loss at epoch[1739]: 0.800\n",
      "Loss at epoch[1749]: 1.597\n",
      "Loss at epoch[1759]: 0.419\n",
      "Loss at epoch[1769]: 0.449\n",
      "Loss at epoch[1779]: 0.716\n",
      "Loss at epoch[1789]: 1.228\n",
      "Loss at epoch[1799]: 1.211\n",
      "Loss at epoch[1809]: 1.076\n",
      "Loss at epoch[1819]: 1.307\n",
      "Loss at epoch[1829]: 1.996\n",
      "Loss at epoch[1839]: 0.632\n",
      "Loss at epoch[1849]: 1.599\n",
      "Loss at epoch[1859]: 1.040\n",
      "Loss at epoch[1869]: 1.815\n",
      "Loss at epoch[1879]: 1.432\n",
      "Loss at epoch[1889]: 0.625\n",
      "Loss at epoch[1899]: 1.000\n",
      "Loss at epoch[1909]: 1.145\n",
      "Loss at epoch[1919]: 0.658\n",
      "Loss at epoch[1929]: 0.907\n",
      "Loss at epoch[1939]: 0.880\n",
      "Loss at epoch[1949]: 3.024\n",
      "Loss at epoch[1959]: 0.390\n",
      "Loss at epoch[1969]: 0.712\n",
      "Loss at epoch[1979]: 1.588\n",
      "Loss at epoch[1989]: 1.228\n",
      "Loss at epoch[1999]: 1.370\n",
      "Loss at epoch[2009]: 1.533\n",
      "Loss at epoch[2019]: 1.427\n",
      "Loss at epoch[2029]: 1.741\n",
      "Loss at epoch[2039]: 1.125\n",
      "Loss at epoch[2049]: 1.563\n",
      "Loss at epoch[2059]: 1.004\n",
      "Loss at epoch[2069]: 1.498\n",
      "Loss at epoch[2079]: 0.935\n",
      "Loss at epoch[2089]: 1.219\n",
      "Loss at epoch[2099]: 2.129\n",
      "Loss at epoch[2109]: 0.575\n",
      "Loss at epoch[2119]: 2.671\n",
      "Loss at epoch[2129]: 0.984\n",
      "Loss at epoch[2139]: 1.194\n",
      "Loss at epoch[2149]: 1.545\n",
      "Loss at epoch[2159]: 0.681\n",
      "Loss at epoch[2169]: 0.484\n",
      "Loss at epoch[2179]: 1.617\n",
      "Loss at epoch[2189]: 1.095\n",
      "Loss at epoch[2199]: 0.631\n",
      "Loss at epoch[2209]: 2.635\n",
      "Loss at epoch[2219]: 1.528\n",
      "Loss at epoch[2229]: 1.471\n",
      "Loss at epoch[2239]: 0.583\n",
      "Loss at epoch[2249]: 0.555\n",
      "Loss at epoch[2259]: 0.408\n",
      "Loss at epoch[2269]: 2.329\n",
      "Loss at epoch[2279]: 1.203\n",
      "Loss at epoch[2289]: 1.152\n",
      "Loss at epoch[2299]: 0.872\n",
      "Loss at epoch[2309]: 1.062\n",
      "Loss at epoch[2319]: 1.291\n",
      "Loss at epoch[2329]: 1.186\n",
      "Loss at epoch[2339]: 0.782\n",
      "Loss at epoch[2349]: 1.469\n",
      "Loss at epoch[2359]: 1.141\n",
      "Loss at epoch[2369]: 0.735\n",
      "Loss at epoch[2379]: 1.293\n",
      "Loss at epoch[2389]: 0.480\n",
      "Loss at epoch[2399]: 0.873\n",
      "Loss at epoch[2409]: 0.772\n",
      "Loss at epoch[2419]: 1.590\n",
      "Loss at epoch[2429]: 1.730\n",
      "Loss at epoch[2439]: 1.387\n",
      "Loss at epoch[2449]: 1.790\n",
      "Loss at epoch[2459]: 0.587\n",
      "Loss at epoch[2469]: 0.450\n",
      "Loss at epoch[2479]: 1.399\n",
      "Loss at epoch[2489]: 1.006\n",
      "Loss at epoch[2499]: 0.572\n",
      "Loss at epoch[2509]: 1.982\n",
      "Loss at epoch[2519]: 1.377\n",
      "Loss at epoch[2529]: 1.086\n",
      "Loss at epoch[2539]: 0.713\n",
      "Loss at epoch[2549]: 1.172\n",
      "Loss at epoch[2559]: 1.318\n",
      "Loss at epoch[2569]: 1.367\n",
      "Loss at epoch[2579]: 0.800\n",
      "Loss at epoch[2589]: 0.854\n",
      "Loss at epoch[2599]: 2.908\n",
      "Loss at epoch[2609]: 0.175\n",
      "Loss at epoch[2619]: 0.633\n",
      "Loss at epoch[2629]: 0.915\n",
      "Loss at epoch[2639]: 0.939\n",
      "Loss at epoch[2649]: 1.045\n",
      "Loss at epoch[2659]: 0.553\n",
      "Loss at epoch[2669]: 0.945\n",
      "Loss at epoch[2679]: 0.707\n",
      "Loss at epoch[2689]: 2.127\n",
      "Loss at epoch[2699]: 1.516\n",
      "Loss at epoch[2709]: 1.138\n",
      "Loss at epoch[2719]: 1.052\n",
      "Loss at epoch[2729]: 0.924\n",
      "Loss at epoch[2739]: 0.845\n",
      "Loss at epoch[2749]: 0.681\n",
      "Loss at epoch[2759]: 1.048\n",
      "Loss at epoch[2769]: 0.782\n",
      "Loss at epoch[2779]: 0.795\n",
      "Loss at epoch[2789]: 0.808\n",
      "Loss at epoch[2799]: 0.646\n",
      "Loss at epoch[2809]: 0.526\n",
      "Loss at epoch[2819]: 0.952\n",
      "Loss at epoch[2829]: 0.746\n",
      "Loss at epoch[2839]: 2.928\n",
      "Loss at epoch[2849]: 0.508\n",
      "Loss at epoch[2859]: 1.254\n",
      "Loss at epoch[2869]: 1.144\n",
      "Loss at epoch[2879]: 1.156\n",
      "Loss at epoch[2889]: 0.256\n",
      "Loss at epoch[2899]: 1.260\n",
      "Loss at epoch[2909]: 1.828\n",
      "Loss at epoch[2919]: 1.963\n",
      "Loss at epoch[2929]: 0.588\n",
      "Loss at epoch[2939]: 2.173\n",
      "Loss at epoch[2949]: 0.891\n",
      "Loss at epoch[2959]: 1.446\n",
      "Loss at epoch[2969]: 1.669\n",
      "Loss at epoch[2979]: 2.550\n",
      "Loss at epoch[2989]: 1.157\n",
      "Loss at epoch[2999]: 0.619\n",
      "Loss at epoch[3009]: 1.871\n",
      "Loss at epoch[3019]: 0.869\n",
      "Loss at epoch[3029]: 1.400\n",
      "Loss at epoch[3039]: 0.663\n",
      "Loss at epoch[3049]: 0.880\n",
      "Loss at epoch[3059]: 0.793\n",
      "Loss at epoch[3069]: 0.576\n",
      "Loss at epoch[3079]: 1.954\n",
      "Loss at epoch[3089]: 1.051\n",
      "Loss at epoch[3099]: 1.009\n",
      "Loss at epoch[3109]: 1.077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch[3119]: 1.718\n",
      "Loss at epoch[3129]: 0.835\n",
      "Loss at epoch[3139]: 1.696\n",
      "Loss at epoch[3149]: 0.759\n",
      "Loss at epoch[3159]: 1.155\n",
      "Loss at epoch[3169]: 0.623\n",
      "Loss at epoch[3179]: 0.324\n",
      "Loss at epoch[3189]: 0.755\n",
      "Loss at epoch[3199]: 0.915\n",
      "Loss at epoch[3209]: 1.109\n",
      "Loss at epoch[3219]: 0.401\n",
      "Loss at epoch[3229]: 1.081\n",
      "Loss at epoch[3239]: 1.289\n",
      "Loss at epoch[3249]: 0.506\n",
      "Loss at epoch[3259]: 0.335\n",
      "Loss at epoch[3269]: 1.176\n",
      "Loss at epoch[3279]: 1.515\n",
      "Loss at epoch[3289]: 0.829\n",
      "Loss at epoch[3299]: 1.079\n",
      "Loss at epoch[3309]: 0.910\n",
      "Loss at epoch[3319]: 0.872\n",
      "Loss at epoch[3329]: 0.632\n",
      "Loss at epoch[3339]: 1.101\n",
      "Loss at epoch[3349]: 0.646\n",
      "Loss at epoch[3359]: 1.195\n",
      "Loss at epoch[3369]: 0.885\n",
      "Loss at epoch[3379]: 0.550\n",
      "Loss at epoch[3389]: 1.409\n",
      "Loss at epoch[3399]: 0.493\n",
      "Loss at epoch[3409]: 0.953\n",
      "Loss at epoch[3419]: 1.263\n",
      "Loss at epoch[3429]: 2.990\n",
      "Loss at epoch[3439]: 1.969\n",
      "Loss at epoch[3449]: 0.689\n",
      "Loss at epoch[3459]: 0.176\n",
      "Loss at epoch[3469]: 0.635\n",
      "Loss at epoch[3479]: 1.458\n",
      "Loss at epoch[3489]: 0.907\n",
      "Loss at epoch[3499]: 1.437\n",
      "Loss at epoch[3509]: 1.125\n",
      "Loss at epoch[3519]: 0.779\n",
      "Loss at epoch[3529]: 1.183\n",
      "Loss at epoch[3539]: 0.391\n",
      "Loss at epoch[3549]: 0.597\n",
      "Loss at epoch[3559]: 1.001\n",
      "Loss at epoch[3569]: 0.246\n",
      "Loss at epoch[3579]: 0.389\n",
      "Loss at epoch[3589]: 1.128\n",
      "Loss at epoch[3599]: 1.975\n",
      "Loss at epoch[3609]: 2.084\n",
      "Loss at epoch[3619]: 0.779\n",
      "Loss at epoch[3629]: 0.206\n",
      "Loss at epoch[3639]: 1.290\n",
      "Loss at epoch[3649]: 1.577\n",
      "Loss at epoch[3659]: 0.587\n",
      "Loss at epoch[3669]: 1.000\n",
      "Loss at epoch[3679]: 1.084\n",
      "Loss at epoch[3689]: 0.471\n",
      "Loss at epoch[3699]: 1.032\n",
      "Loss at epoch[3709]: 0.683\n",
      "Loss at epoch[3719]: 1.081\n",
      "Loss at epoch[3729]: 0.622\n",
      "Loss at epoch[3739]: 0.620\n",
      "Loss at epoch[3749]: 1.615\n",
      "Loss at epoch[3759]: 2.541\n",
      "Loss at epoch[3769]: 1.073\n",
      "Loss at epoch[3779]: 0.349\n",
      "Loss at epoch[3789]: 0.964\n",
      "Loss at epoch[3799]: 1.240\n",
      "Loss at epoch[3809]: 0.840\n",
      "Loss at epoch[3819]: 1.098\n",
      "Loss at epoch[3829]: 0.564\n",
      "Loss at epoch[3839]: 1.274\n",
      "Loss at epoch[3849]: 0.658\n",
      "Loss at epoch[3859]: 1.192\n",
      "Loss at epoch[3869]: 0.836\n",
      "Loss at epoch[3879]: 1.388\n",
      "Loss at epoch[3889]: 0.863\n",
      "Loss at epoch[3899]: 1.118\n",
      "Loss at epoch[3909]: 1.231\n",
      "Loss at epoch[3919]: 0.393\n",
      "Loss at epoch[3929]: 0.933\n",
      "Loss at epoch[3939]: 1.462\n",
      "Loss at epoch[3949]: 0.436\n",
      "Loss at epoch[3959]: 0.679\n",
      "Loss at epoch[3969]: 0.723\n",
      "Loss at epoch[3979]: 0.832\n",
      "Loss at epoch[3989]: 0.558\n",
      "Loss at epoch[3999]: 0.897\n",
      "Loss at epoch[4009]: 0.605\n",
      "Loss at epoch[4019]: 0.620\n",
      "Loss at epoch[4029]: 1.088\n",
      "Loss at epoch[4039]: 2.176\n",
      "Loss at epoch[4049]: 1.566\n",
      "Loss at epoch[4059]: 3.044\n",
      "Loss at epoch[4069]: 1.027\n",
      "Loss at epoch[4079]: 1.114\n",
      "Loss at epoch[4089]: 1.056\n",
      "Loss at epoch[4099]: 1.046\n",
      "Loss at epoch[4109]: 1.825\n",
      "Loss at epoch[4119]: 0.531\n",
      "Loss at epoch[4129]: 1.633\n",
      "Loss at epoch[4139]: 1.003\n",
      "Loss at epoch[4149]: 1.010\n",
      "Loss at epoch[4159]: 0.480\n",
      "Loss at epoch[4169]: 1.768\n",
      "Loss at epoch[4179]: 1.071\n",
      "Loss at epoch[4189]: 2.481\n",
      "Loss at epoch[4199]: 0.468\n",
      "Loss at epoch[4209]: 0.784\n",
      "Loss at epoch[4219]: 0.901\n",
      "Loss at epoch[4229]: 1.188\n",
      "Loss at epoch[4239]: 0.511\n",
      "Loss at epoch[4249]: 0.525\n",
      "Loss at epoch[4259]: 2.579\n",
      "Loss at epoch[4269]: 1.293\n",
      "Loss at epoch[4279]: 0.681\n",
      "Loss at epoch[4289]: 0.516\n",
      "Loss at epoch[4299]: 0.678\n",
      "Loss at epoch[4309]: 0.537\n",
      "Loss at epoch[4319]: 1.605\n",
      "Loss at epoch[4329]: 1.361\n",
      "Loss at epoch[4339]: 0.725\n",
      "Loss at epoch[4349]: 0.308\n",
      "Loss at epoch[4359]: 0.965\n",
      "Loss at epoch[4369]: 1.396\n",
      "Loss at epoch[4379]: 0.910\n",
      "Loss at epoch[4389]: 1.347\n",
      "Loss at epoch[4399]: 1.272\n",
      "Loss at epoch[4409]: 1.087\n",
      "Loss at epoch[4419]: 0.872\n",
      "Loss at epoch[4429]: 0.289\n",
      "Loss at epoch[4439]: 1.185\n",
      "Loss at epoch[4449]: 1.100\n",
      "Loss at epoch[4459]: 0.718\n",
      "Loss at epoch[4469]: 1.223\n",
      "Loss at epoch[4479]: 0.882\n",
      "Loss at epoch[4489]: 1.074\n",
      "Loss at epoch[4499]: 0.847\n",
      "Loss at epoch[4509]: 1.129\n",
      "Loss at epoch[4519]: 0.941\n",
      "Loss at epoch[4529]: 1.410\n",
      "Loss at epoch[4539]: 1.793\n",
      "Loss at epoch[4549]: 1.332\n",
      "Loss at epoch[4559]: 1.030\n",
      "Loss at epoch[4569]: 1.062\n",
      "Loss at epoch[4579]: 1.016\n",
      "Loss at epoch[4589]: 0.833\n",
      "Loss at epoch[4599]: 0.215\n",
      "Loss at epoch[4609]: 0.666\n",
      "Loss at epoch[4619]: 1.055\n",
      "Loss at epoch[4629]: 1.083\n",
      "Loss at epoch[4639]: 0.542\n",
      "Loss at epoch[4649]: 2.926\n",
      "Loss at epoch[4659]: 0.599\n",
      "Loss at epoch[4669]: 1.137\n",
      "Loss at epoch[4679]: 1.122\n",
      "Loss at epoch[4689]: 0.753\n",
      "Loss at epoch[4699]: 0.891\n",
      "Loss at epoch[4709]: 2.055\n",
      "Loss at epoch[4719]: 2.345\n",
      "Loss at epoch[4729]: 0.581\n",
      "Loss at epoch[4739]: 0.955\n",
      "Loss at epoch[4749]: 1.400\n",
      "Loss at epoch[4759]: 1.283\n",
      "Loss at epoch[4769]: 1.179\n",
      "Loss at epoch[4779]: 1.744\n",
      "Loss at epoch[4789]: 0.697\n",
      "Loss at epoch[4799]: 1.094\n",
      "Loss at epoch[4809]: 1.282\n",
      "Loss at epoch[4819]: 0.960\n",
      "Loss at epoch[4829]: 0.187\n",
      "Loss at epoch[4839]: 1.223\n",
      "Loss at epoch[4849]: 0.623\n",
      "Loss at epoch[4859]: 1.028\n",
      "Loss at epoch[4869]: 1.581\n",
      "Loss at epoch[4879]: 0.707\n",
      "Loss at epoch[4889]: 0.358\n",
      "Loss at epoch[4899]: 0.846\n",
      "Loss at epoch[4909]: 0.843\n",
      "Loss at epoch[4919]: 1.907\n",
      "Loss at epoch[4929]: 0.199\n",
      "Loss at epoch[4939]: 1.246\n",
      "Loss at epoch[4949]: 1.052\n",
      "Loss at epoch[4959]: 0.756\n",
      "Loss at epoch[4969]: 0.374\n",
      "Loss at epoch[4979]: 0.955\n",
      "Loss at epoch[4989]: 2.040\n",
      "Loss at epoch[4999]: 0.829\n",
      "Loss at epoch[5009]: 0.553\n",
      "Loss at epoch[5019]: 0.913\n",
      "Loss at epoch[5029]: 2.211\n",
      "Loss at epoch[5039]: 0.418\n",
      "Loss at epoch[5049]: 2.358\n",
      "Loss at epoch[5059]: 0.357\n",
      "Loss at epoch[5069]: 0.392\n",
      "Loss at epoch[5079]: 1.003\n",
      "Loss at epoch[5089]: 1.381\n",
      "Loss at epoch[5099]: 1.508\n",
      "Loss at epoch[5109]: 1.425\n",
      "Loss at epoch[5119]: 0.843\n",
      "Loss at epoch[5129]: 1.026\n",
      "Loss at epoch[5139]: 1.109\n",
      "Loss at epoch[5149]: 1.303\n",
      "Loss at epoch[5159]: 0.532\n",
      "Loss at epoch[5169]: 1.450\n",
      "Loss at epoch[5179]: 1.004\n",
      "Loss at epoch[5189]: 1.052\n",
      "Loss at epoch[5199]: 1.704\n",
      "Loss at epoch[5209]: 1.014\n",
      "Loss at epoch[5219]: 1.578\n",
      "Loss at epoch[5229]: 0.395\n",
      "Loss at epoch[5239]: 1.196\n",
      "Loss at epoch[5249]: 1.715\n",
      "Loss at epoch[5259]: 0.638\n",
      "Loss at epoch[5269]: 0.438\n",
      "Loss at epoch[5279]: 0.539\n",
      "Loss at epoch[5289]: 1.028\n",
      "Loss at epoch[5299]: 1.551\n",
      "Loss at epoch[5309]: 1.839\n",
      "Loss at epoch[5319]: 1.392\n",
      "Loss at epoch[5329]: 0.388\n",
      "Loss at epoch[5339]: 0.774\n",
      "Loss at epoch[5349]: 1.009\n",
      "Loss at epoch[5359]: 1.015\n",
      "Loss at epoch[5369]: 1.181\n",
      "Loss at epoch[5379]: 1.400\n",
      "Loss at epoch[5389]: 1.705\n",
      "Loss at epoch[5399]: 0.761\n",
      "Loss at epoch[5409]: 0.572\n",
      "Loss at epoch[5419]: 0.657\n",
      "Loss at epoch[5429]: 1.699\n",
      "Loss at epoch[5439]: 0.565\n",
      "Loss at epoch[5449]: 1.821\n",
      "Loss at epoch[5459]: 0.616\n",
      "Loss at epoch[5469]: 1.708\n",
      "Loss at epoch[5479]: 1.175\n",
      "Loss at epoch[5489]: 2.016\n",
      "Loss at epoch[5499]: 0.630\n",
      "Loss at epoch[5509]: 0.646\n",
      "Loss at epoch[5519]: 0.384\n",
      "Loss at epoch[5529]: 0.652\n",
      "Loss at epoch[5539]: 0.853\n",
      "Loss at epoch[5549]: 0.796\n",
      "Loss at epoch[5559]: 0.541\n",
      "Loss at epoch[5569]: 0.687\n",
      "Loss at epoch[5579]: 1.339\n",
      "Loss at epoch[5589]: 0.442\n",
      "Loss at epoch[5599]: 0.289\n",
      "Loss at epoch[5609]: 1.431\n",
      "Loss at epoch[5619]: 0.659\n",
      "Loss at epoch[5629]: 1.066\n",
      "Loss at epoch[5639]: 0.537\n",
      "Loss at epoch[5649]: 0.810\n",
      "Loss at epoch[5659]: 1.424\n",
      "Loss at epoch[5669]: 0.696\n",
      "Loss at epoch[5679]: 0.775\n",
      "Loss at epoch[5689]: 0.898\n",
      "Loss at epoch[5699]: 0.828\n",
      "Loss at epoch[5709]: 1.624\n",
      "Loss at epoch[5719]: 1.583\n",
      "Loss at epoch[5729]: 0.678\n",
      "Loss at epoch[5739]: 0.345\n",
      "Loss at epoch[5749]: 1.743\n",
      "Loss at epoch[5759]: 0.867\n",
      "Loss at epoch[5769]: 1.340\n",
      "Loss at epoch[5779]: 0.710\n",
      "Loss at epoch[5789]: 1.341\n",
      "Loss at epoch[5799]: 2.450\n",
      "Loss at epoch[5809]: 1.773\n",
      "Loss at epoch[5819]: 3.559\n",
      "Loss at epoch[5829]: 0.700\n",
      "Loss at epoch[5839]: 0.516\n",
      "Loss at epoch[5849]: 1.154\n",
      "Loss at epoch[5859]: 0.859\n",
      "Loss at epoch[5869]: 1.415\n",
      "Loss at epoch[5879]: 1.229\n",
      "Loss at epoch[5889]: 0.880\n",
      "Loss at epoch[5899]: 0.832\n",
      "Loss at epoch[5909]: 0.292\n",
      "Loss at epoch[5919]: 2.085\n",
      "Loss at epoch[5929]: 1.251\n",
      "Loss at epoch[5939]: 0.543\n",
      "Loss at epoch[5949]: 1.272\n",
      "Loss at epoch[5959]: 0.658\n",
      "Loss at epoch[5969]: 0.666\n",
      "Loss at epoch[5979]: 1.684\n",
      "Loss at epoch[5989]: 0.683\n",
      "Loss at epoch[5999]: 1.046\n",
      "Loss at epoch[6009]: 1.520\n",
      "Loss at epoch[6019]: 0.809\n",
      "Loss at epoch[6029]: 1.883\n",
      "Loss at epoch[6039]: 0.558\n",
      "Loss at epoch[6049]: 0.988\n",
      "Loss at epoch[6059]: 2.097\n",
      "Loss at epoch[6069]: 1.226\n",
      "Loss at epoch[6079]: 0.418\n",
      "Loss at epoch[6089]: 0.607\n",
      "Loss at epoch[6099]: 2.549\n",
      "Loss at epoch[6109]: 0.769\n",
      "Loss at epoch[6119]: 1.877\n",
      "Loss at epoch[6129]: 0.645\n",
      "Loss at epoch[6139]: 1.325\n",
      "Loss at epoch[6149]: 0.966\n",
      "Loss at epoch[6159]: 0.772\n",
      "Loss at epoch[6169]: 1.129\n",
      "Loss at epoch[6179]: 0.698\n",
      "Loss at epoch[6189]: 0.693\n",
      "Loss at epoch[6199]: 0.676\n",
      "Loss at epoch[6209]: 0.610\n",
      "Loss at epoch[6219]: 1.031\n",
      "Loss at epoch[6229]: 0.713\n",
      "Loss at epoch[6239]: 1.083\n",
      "Loss at epoch[6249]: 0.478\n",
      "Loss at epoch[6259]: 0.337\n",
      "Loss at epoch[6269]: 0.627\n",
      "Loss at epoch[6279]: 0.779\n",
      "Loss at epoch[6289]: 0.857\n",
      "Loss at epoch[6299]: 0.726\n",
      "Loss at epoch[6309]: 0.666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch[6319]: 0.868\n",
      "Loss at epoch[6329]: 0.543\n",
      "Loss at epoch[6339]: 1.098\n",
      "Loss at epoch[6349]: 0.677\n",
      "Loss at epoch[6359]: 0.694\n",
      "Loss at epoch[6369]: 1.093\n",
      "Loss at epoch[6379]: 0.199\n",
      "Loss at epoch[6389]: 0.848\n",
      "Loss at epoch[6399]: 0.783\n",
      "Loss at epoch[6409]: 0.649\n",
      "Loss at epoch[6419]: 0.572\n",
      "Loss at epoch[6429]: 1.060\n",
      "Loss at epoch[6439]: 1.300\n",
      "Loss at epoch[6449]: 1.517\n",
      "Loss at epoch[6459]: 0.914\n",
      "Loss at epoch[6469]: 0.962\n",
      "Loss at epoch[6479]: 0.477\n",
      "Loss at epoch[6489]: 0.351\n",
      "Loss at epoch[6499]: 1.883\n",
      "Loss at epoch[6509]: 0.412\n",
      "Loss at epoch[6519]: 0.617\n",
      "Loss at epoch[6529]: 1.153\n",
      "Loss at epoch[6539]: 1.916\n",
      "Loss at epoch[6549]: 0.895\n",
      "Loss at epoch[6559]: 0.381\n",
      "Loss at epoch[6569]: 1.779\n",
      "Loss at epoch[6579]: 0.533\n",
      "Loss at epoch[6589]: 1.342\n",
      "Loss at epoch[6599]: 1.185\n",
      "Loss at epoch[6609]: 2.019\n",
      "Loss at epoch[6619]: 0.480\n",
      "Loss at epoch[6629]: 0.885\n",
      "Loss at epoch[6639]: 1.004\n",
      "Loss at epoch[6649]: 0.398\n",
      "Loss at epoch[6659]: 0.694\n",
      "Loss at epoch[6669]: 0.766\n",
      "Loss at epoch[6679]: 1.103\n",
      "Loss at epoch[6689]: 0.589\n",
      "Loss at epoch[6699]: 0.752\n",
      "Loss at epoch[6709]: 1.169\n",
      "Loss at epoch[6719]: 1.021\n",
      "Loss at epoch[6729]: 0.649\n",
      "Loss at epoch[6739]: 1.707\n",
      "Loss at epoch[6749]: 1.145\n",
      "Loss at epoch[6759]: 0.928\n",
      "Loss at epoch[6769]: 0.742\n",
      "Loss at epoch[6779]: 0.904\n",
      "Loss at epoch[6789]: 0.974\n",
      "Loss at epoch[6799]: 1.540\n",
      "Loss at epoch[6809]: 1.369\n",
      "Loss at epoch[6819]: 0.917\n",
      "Loss at epoch[6829]: 1.077\n",
      "Loss at epoch[6839]: 0.873\n",
      "Loss at epoch[6849]: 1.330\n",
      "Loss at epoch[6859]: 0.527\n",
      "Loss at epoch[6869]: 1.009\n",
      "Loss at epoch[6879]: 0.864\n",
      "Loss at epoch[6889]: 1.032\n",
      "Loss at epoch[6899]: 0.853\n",
      "Loss at epoch[6909]: 2.742\n",
      "Loss at epoch[6919]: 0.929\n",
      "Loss at epoch[6929]: 1.237\n",
      "Loss at epoch[6939]: 1.712\n",
      "Loss at epoch[6949]: 1.617\n",
      "Loss at epoch[6959]: 0.650\n",
      "Loss at epoch[6969]: 0.902\n",
      "Loss at epoch[6979]: 1.385\n",
      "Loss at epoch[6989]: 1.157\n",
      "Loss at epoch[6999]: 0.363\n",
      "Loss at epoch[7009]: 0.457\n",
      "Loss at epoch[7019]: 0.376\n",
      "Loss at epoch[7029]: 0.581\n",
      "Loss at epoch[7039]: 0.607\n",
      "Loss at epoch[7049]: 0.969\n",
      "Loss at epoch[7059]: 1.433\n",
      "Loss at epoch[7069]: 1.307\n",
      "Loss at epoch[7079]: 1.500\n",
      "Loss at epoch[7089]: 0.846\n",
      "Loss at epoch[7099]: 0.865\n",
      "Loss at epoch[7109]: 1.652\n",
      "Loss at epoch[7119]: 1.783\n",
      "Loss at epoch[7129]: 0.892\n",
      "Loss at epoch[7139]: 0.503\n",
      "Loss at epoch[7149]: 0.366\n",
      "Loss at epoch[7159]: 0.810\n",
      "Loss at epoch[7169]: 0.593\n",
      "Loss at epoch[7179]: 0.723\n",
      "Loss at epoch[7189]: 2.071\n",
      "Loss at epoch[7199]: 0.998\n",
      "Loss at epoch[7209]: 3.021\n",
      "Loss at epoch[7219]: 1.192\n",
      "Loss at epoch[7229]: 0.634\n",
      "Loss at epoch[7239]: 0.807\n",
      "Loss at epoch[7249]: 0.420\n",
      "Loss at epoch[7259]: 1.891\n",
      "Loss at epoch[7269]: 1.403\n",
      "Loss at epoch[7279]: 0.886\n",
      "Loss at epoch[7289]: 0.926\n",
      "Loss at epoch[7299]: 1.388\n",
      "Loss at epoch[7309]: 1.994\n",
      "Loss at epoch[7319]: 0.951\n",
      "Loss at epoch[7329]: 0.668\n",
      "Loss at epoch[7339]: 0.882\n",
      "Loss at epoch[7349]: 1.150\n",
      "Loss at epoch[7359]: 1.479\n",
      "Loss at epoch[7369]: 1.133\n",
      "Loss at epoch[7379]: 1.085\n",
      "Loss at epoch[7389]: 1.044\n",
      "Loss at epoch[7399]: 0.929\n",
      "Loss at epoch[7409]: 0.607\n",
      "Loss at epoch[7419]: 0.873\n",
      "Loss at epoch[7429]: 1.263\n",
      "Loss at epoch[7439]: 1.124\n",
      "Loss at epoch[7449]: 0.724\n",
      "Loss at epoch[7459]: 0.855\n",
      "Loss at epoch[7469]: 1.504\n",
      "Loss at epoch[7479]: 1.282\n",
      "Loss at epoch[7489]: 0.730\n",
      "Loss at epoch[7499]: 1.176\n",
      "Loss at epoch[7509]: 1.108\n",
      "Loss at epoch[7519]: 0.679\n",
      "Loss at epoch[7529]: 2.675\n",
      "Loss at epoch[7539]: 0.704\n",
      "Loss at epoch[7549]: 1.310\n",
      "Loss at epoch[7559]: 2.564\n",
      "Loss at epoch[7569]: 0.421\n",
      "Loss at epoch[7579]: 1.118\n",
      "Loss at epoch[7589]: 1.533\n",
      "Loss at epoch[7599]: 0.651\n",
      "Loss at epoch[7609]: 1.119\n",
      "Loss at epoch[7619]: 1.698\n",
      "Loss at epoch[7629]: 1.219\n",
      "Loss at epoch[7639]: 1.264\n",
      "Loss at epoch[7649]: 2.088\n",
      "Loss at epoch[7659]: 0.698\n",
      "Loss at epoch[7669]: 0.907\n",
      "Loss at epoch[7679]: 0.881\n",
      "Loss at epoch[7689]: 1.404\n",
      "Loss at epoch[7699]: 0.573\n",
      "Loss at epoch[7709]: 1.369\n",
      "Loss at epoch[7719]: 1.159\n",
      "Loss at epoch[7729]: 0.853\n",
      "Loss at epoch[7739]: 0.234\n",
      "Loss at epoch[7749]: 1.197\n",
      "Loss at epoch[7759]: 1.174\n",
      "Loss at epoch[7769]: 1.718\n",
      "Loss at epoch[7779]: 1.105\n",
      "Loss at epoch[7789]: 1.273\n",
      "Loss at epoch[7799]: 1.276\n",
      "Loss at epoch[7809]: 1.535\n",
      "Loss at epoch[7819]: 1.513\n",
      "Loss at epoch[7829]: 0.279\n",
      "Loss at epoch[7839]: 1.059\n",
      "Loss at epoch[7849]: 0.799\n",
      "Loss at epoch[7859]: 0.991\n",
      "Loss at epoch[7869]: 1.653\n",
      "Loss at epoch[7879]: 1.150\n",
      "Loss at epoch[7889]: 0.428\n",
      "Loss at epoch[7899]: 0.266\n",
      "Loss at epoch[7909]: 0.846\n",
      "Loss at epoch[7919]: 0.933\n",
      "Loss at epoch[7929]: 2.123\n",
      "Loss at epoch[7939]: 0.478\n",
      "Loss at epoch[7949]: 0.825\n",
      "Loss at epoch[7959]: 1.957\n",
      "Loss at epoch[7969]: 0.671\n",
      "Loss at epoch[7979]: 0.374\n",
      "Loss at epoch[7989]: 2.026\n",
      "Loss at epoch[7999]: 1.208\n",
      "Loss at epoch[8009]: 1.842\n",
      "Loss at epoch[8019]: 1.056\n",
      "Loss at epoch[8029]: 0.637\n",
      "Loss at epoch[8039]: 1.011\n",
      "Loss at epoch[8049]: 1.288\n",
      "Loss at epoch[8059]: 0.847\n",
      "Loss at epoch[8069]: 1.042\n",
      "Loss at epoch[8079]: 0.743\n",
      "Loss at epoch[8089]: 1.668\n",
      "Loss at epoch[8099]: 1.508\n",
      "Loss at epoch[8109]: 0.483\n",
      "Loss at epoch[8119]: 2.457\n",
      "Loss at epoch[8129]: 1.704\n",
      "Loss at epoch[8139]: 1.255\n",
      "Loss at epoch[8149]: 0.807\n",
      "Loss at epoch[8159]: 0.746\n",
      "Loss at epoch[8169]: 0.542\n",
      "Loss at epoch[8179]: 1.179\n",
      "Loss at epoch[8189]: 0.409\n",
      "Loss at epoch[8199]: 0.966\n",
      "Loss at epoch[8209]: 1.778\n",
      "Loss at epoch[8219]: 0.726\n",
      "Loss at epoch[8229]: 1.333\n",
      "Loss at epoch[8239]: 0.582\n",
      "Loss at epoch[8249]: 1.749\n",
      "Loss at epoch[8259]: 0.338\n",
      "Loss at epoch[8269]: 0.898\n",
      "Loss at epoch[8279]: 1.396\n",
      "Loss at epoch[8289]: 1.514\n",
      "Loss at epoch[8299]: 0.797\n",
      "Loss at epoch[8309]: 0.924\n",
      "Loss at epoch[8319]: 1.072\n",
      "Loss at epoch[8329]: 0.487\n",
      "Loss at epoch[8339]: 2.034\n",
      "Loss at epoch[8349]: 1.810\n",
      "Loss at epoch[8359]: 0.462\n",
      "Loss at epoch[8369]: 0.785\n",
      "Loss at epoch[8379]: 0.557\n",
      "Loss at epoch[8389]: 1.021\n",
      "Loss at epoch[8399]: 2.610\n",
      "Loss at epoch[8409]: 1.662\n",
      "Loss at epoch[8419]: 0.952\n",
      "Loss at epoch[8429]: 1.238\n",
      "Loss at epoch[8439]: 1.090\n",
      "Loss at epoch[8449]: 2.122\n",
      "Loss at epoch[8459]: 0.406\n",
      "Loss at epoch[8469]: 2.705\n",
      "Loss at epoch[8479]: 0.870\n",
      "Loss at epoch[8489]: 0.711\n",
      "Loss at epoch[8499]: 0.880\n",
      "Loss at epoch[8509]: 0.491\n",
      "Loss at epoch[8519]: 0.417\n",
      "Loss at epoch[8529]: 0.784\n",
      "Loss at epoch[8539]: 1.250\n",
      "Loss at epoch[8549]: 0.932\n",
      "Loss at epoch[8559]: 1.107\n",
      "Loss at epoch[8569]: 0.610\n",
      "Loss at epoch[8579]: 2.575\n",
      "Loss at epoch[8589]: 0.931\n",
      "Loss at epoch[8599]: 1.516\n",
      "Loss at epoch[8609]: 0.676\n",
      "Loss at epoch[8619]: 1.360\n",
      "Loss at epoch[8629]: 0.711\n",
      "Loss at epoch[8639]: 1.715\n",
      "Loss at epoch[8649]: 0.741\n",
      "Loss at epoch[8659]: 0.705\n",
      "Loss at epoch[8669]: 0.850\n",
      "Loss at epoch[8679]: 2.584\n",
      "Loss at epoch[8689]: 1.920\n",
      "Loss at epoch[8699]: 1.159\n",
      "Loss at epoch[8709]: 0.851\n",
      "Loss at epoch[8719]: 1.489\n",
      "Loss at epoch[8729]: 2.209\n",
      "Loss at epoch[8739]: 1.220\n",
      "Loss at epoch[8749]: 1.592\n",
      "Loss at epoch[8759]: 2.997\n",
      "Loss at epoch[8769]: 1.198\n",
      "Loss at epoch[8779]: 1.207\n",
      "Loss at epoch[8789]: 0.530\n",
      "Loss at epoch[8799]: 0.925\n",
      "Loss at epoch[8809]: 1.662\n",
      "Loss at epoch[8819]: 0.468\n",
      "Loss at epoch[8829]: 0.648\n",
      "Loss at epoch[8839]: 0.891\n",
      "Loss at epoch[8849]: 1.137\n",
      "Loss at epoch[8859]: 1.326\n",
      "Loss at epoch[8869]: 0.864\n",
      "Loss at epoch[8879]: 1.423\n",
      "Loss at epoch[8889]: 1.253\n",
      "Loss at epoch[8899]: 1.258\n",
      "Loss at epoch[8909]: 0.712\n",
      "Loss at epoch[8919]: 2.325\n",
      "Loss at epoch[8929]: 0.345\n",
      "Loss at epoch[8939]: 0.363\n",
      "Loss at epoch[8949]: 0.805\n",
      "Loss at epoch[8959]: 0.836\n",
      "Loss at epoch[8969]: 2.159\n",
      "Loss at epoch[8979]: 0.615\n",
      "Loss at epoch[8989]: 1.188\n",
      "Loss at epoch[8999]: 0.868\n",
      "Loss at epoch[9009]: 0.734\n",
      "Loss at epoch[9019]: 1.738\n",
      "Loss at epoch[9029]: 1.021\n",
      "Loss at epoch[9039]: 2.064\n",
      "Loss at epoch[9049]: 0.564\n",
      "Loss at epoch[9059]: 1.899\n",
      "Loss at epoch[9069]: 0.477\n",
      "Loss at epoch[9079]: 0.344\n",
      "Loss at epoch[9089]: 1.860\n",
      "Loss at epoch[9099]: 1.322\n",
      "Loss at epoch[9109]: 0.403\n",
      "Loss at epoch[9119]: 1.496\n",
      "Loss at epoch[9129]: 1.975\n",
      "Loss at epoch[9139]: 0.661\n",
      "Loss at epoch[9149]: 0.907\n",
      "Loss at epoch[9159]: 0.802\n",
      "Loss at epoch[9169]: 0.642\n",
      "Loss at epoch[9179]: 0.784\n",
      "Loss at epoch[9189]: 1.329\n",
      "Loss at epoch[9199]: 0.719\n",
      "Loss at epoch[9209]: 0.316\n",
      "Loss at epoch[9219]: 1.580\n",
      "Loss at epoch[9229]: 0.311\n",
      "Loss at epoch[9239]: 2.391\n",
      "Loss at epoch[9249]: 0.319\n",
      "Loss at epoch[9259]: 1.618\n",
      "Loss at epoch[9269]: 1.693\n",
      "Loss at epoch[9279]: 0.540\n",
      "Loss at epoch[9289]: 0.457\n",
      "Loss at epoch[9299]: 0.495\n",
      "Loss at epoch[9309]: 1.327\n",
      "Loss at epoch[9319]: 0.918\n",
      "Loss at epoch[9329]: 0.628\n",
      "Loss at epoch[9339]: 0.658\n",
      "Loss at epoch[9349]: 1.300\n",
      "Loss at epoch[9359]: 0.761\n",
      "Loss at epoch[9369]: 1.131\n",
      "Loss at epoch[9379]: 1.318\n",
      "Loss at epoch[9389]: 0.612\n",
      "Loss at epoch[9399]: 0.507\n",
      "Loss at epoch[9409]: 1.103\n",
      "Loss at epoch[9419]: 0.441\n",
      "Loss at epoch[9429]: 1.088\n",
      "Loss at epoch[9439]: 1.501\n",
      "Loss at epoch[9449]: 1.254\n",
      "Loss at epoch[9459]: 0.443\n",
      "Loss at epoch[9469]: 0.413\n",
      "Loss at epoch[9479]: 0.782\n",
      "Loss at epoch[9489]: 1.513\n",
      "Loss at epoch[9499]: 1.170\n",
      "Loss at epoch[9509]: 1.867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch[9519]: 0.779\n",
      "Loss at epoch[9529]: 0.609\n",
      "Loss at epoch[9539]: 1.669\n",
      "Loss at epoch[9549]: 1.085\n",
      "Loss at epoch[9559]: 0.916\n",
      "Loss at epoch[9569]: 0.934\n",
      "Loss at epoch[9579]: 2.907\n",
      "Loss at epoch[9589]: 0.494\n",
      "Loss at epoch[9599]: 1.538\n",
      "Loss at epoch[9609]: 1.069\n",
      "Loss at epoch[9619]: 1.487\n",
      "Loss at epoch[9629]: 2.050\n",
      "Loss at epoch[9639]: 1.281\n",
      "Loss at epoch[9649]: 2.040\n",
      "Loss at epoch[9659]: 0.412\n",
      "Loss at epoch[9669]: 0.737\n",
      "Loss at epoch[9679]: 1.877\n",
      "Loss at epoch[9689]: 1.533\n",
      "Loss at epoch[9699]: 0.684\n",
      "Loss at epoch[9709]: 0.738\n",
      "Loss at epoch[9719]: 1.173\n",
      "Loss at epoch[9729]: 0.943\n",
      "Loss at epoch[9739]: 1.106\n",
      "Loss at epoch[9749]: 0.490\n",
      "Loss at epoch[9759]: 0.677\n",
      "Loss at epoch[9769]: 1.309\n",
      "Loss at epoch[9779]: 1.180\n",
      "Loss at epoch[9789]: 0.997\n",
      "Loss at epoch[9799]: 0.704\n",
      "Loss at epoch[9809]: 1.653\n",
      "Loss at epoch[9819]: 0.827\n",
      "Loss at epoch[9829]: 2.657\n",
      "Loss at epoch[9839]: 1.750\n",
      "Loss at epoch[9849]: 0.730\n",
      "Loss at epoch[9859]: 1.264\n",
      "Loss at epoch[9869]: 1.111\n",
      "Loss at epoch[9879]: 2.014\n",
      "Loss at epoch[9889]: 1.863\n",
      "Loss at epoch[9899]: 0.739\n",
      "Loss at epoch[9909]: 1.021\n",
      "Loss at epoch[9919]: 0.779\n",
      "Loss at epoch[9929]: 0.689\n",
      "Loss at epoch[9939]: 1.236\n",
      "Loss at epoch[9949]: 0.573\n",
      "Loss at epoch[9959]: 1.004\n",
      "Loss at epoch[9969]: 0.433\n",
      "Loss at epoch[9979]: 0.391\n",
      "Loss at epoch[9989]: 0.979\n",
      "Loss at epoch[9999]: 0.848\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEKCAYAAAACS67iAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAGQRJREFUeJzt3X+QH3V9x/HXK5cEucSRcNzQSEgOlGmHdtogVwZGx7FE\nK1KrdMZ2pKdNW9orsc74o9MKXqfVmd6MdlSsraDXAmZ6V5UiLZahYzHSsWobeykBAkgJmMSkgRxa\nSvE6KvDuH7vHfe+b736/+/29373nY2bndj/f/e6+d/d7r+9+97vfXUeEAACDb02/CwAAdAaBDgAl\nQaADQEkQ6ABQEgQ6AJQEgQ4AJUGgA0BJ5A5020O277F9Rzp8ju29tg/a/rzt9d0rEwDQSDN76O+S\n9FDF8IclXRcRL5f035Ku6mRhAIDmOM8vRW1vkbRb0rSk90r6RUkLkn4sIp61fYmkD0TE6+tN54wz\nzoixsbG2iwaA1WTfvn1PRsRoo/HW5pzexyX9gaQXp8Mjkp6KiGfT4aOSzmo0kbGxMc3Pz+ecJQBA\nkmwfzjNew0Mutt8o6URE7GuxkEnb87bnFxYWWpkEACCHPMfQXynpTbYPSfqcpEsl/Zmk02wv7eFv\nkXSs1pMjYiYixiNifHS04ScGAECLGgZ6RFwbEVsiYkzSWyV9JSImJN0t6S3paDsl3d61KgEADbVz\nHvr7JL3X9kElx9Rv7ExJAIBW5P1SVJIUEf8s6Z/T/sckXdT5kgAArSj+L0Xn5qSxMWnNmuTv3Fy/\nKwKAQmpqD73n5uakyUlpcTEZPnw4GZakiYn+1QUABVTsPfSpqeUwX7K4mLQDAFYodqAfOdJcOwCs\nYsUO9K1bm2sHgFWs2IE+PS0ND69sGx5O2gEAKxQ70CcmpJkZaePGZHjbtmSYL0QB4CTFPstFSsL7\nG9+QbrlFOnSo39UAQGEVew+9Uo7L/ALAajYYgW73uwIAKLzBCHQAQEMEOgCUBIEOACVBoANASQxO\noHOWCwDUNRiBzlkuANDQYAQ6AKChhoFu+0W2v2n7XtsP2P5g2v4Z29+2vT/ttne/XABAljw//f+B\npEsj4hnb6yR9zfY/po/9fkTc2r3yAAB5NQz0iAhJz6SD69KObygBoGByHUO3PWR7v6QTku6KiL3p\nQ9O277N9ne1TulalxFkuANBArkCPiOciYrukLZIusv1Tkq6V9BOSflbS6ZLeV+u5tidtz9ueX1hY\naK1KznIBgIaaOsslIp6SdLekyyLieCR+IOlmSRdlPGcmIsYjYnx0dLT9igEANeU5y2XU9mlp/6mS\nXifpW7Y3p22WdIWkA90sFABQX56zXDZL2m17SMkbwC0RcYftr9gelWRJ+yVd3cU6AQAN5DnL5T5J\nF9Rov7QrFQEAWsIvRQGgJAYn0DltEQDqGoxA57RFAGhoMAIdANBQ8QN9bk66+WbpqaeksbFkGABw\nkjynLfbP3Jw0OSktLibDhw8nw5I0MdG/ugCggIq9hz41tRzmSxYXk3YAwArFDvQjR5prB4BVrNiB\nvnVrc+0AsIoVO9Cnp6Xh4ZVtw8NJOwBghWIH+sSENDOzPLxlSzLMF6IAcJJiB7q0Mrz37yfMASBD\n8QMdAJALgQ4AJUGgA0BJDFagc8VFAMg0WIEOAMiU556iL7L9Tdv32n7A9gfT9nNs77V90Pbnba/v\nfrkAgCx59tB/IOnSiPgZSdslXWb7YkkflnRdRLxc0n9Luqp7ZQIAGmkY6JF4Jh1cl3Yh6VJJt6bt\nuyVd0ZUKAQC55DqGbnvI9n5JJyTdJelRSU9FxLPpKEclnZXx3Enb87bnFxYWOlEzAKCGXIEeEc9F\nxHZJWyRdJOkn8s4gImYiYjwixkdHR1ssEwDQSFNnuUTEU5LulnSJpNNsL90gY4ukYx2u7WTcWxQA\nMuU5y2XU9mlp/6mSXifpISXB/pZ0tJ2Sbu9WkQCAxvLcgm6zpN22h5S8AdwSEXfYflDS52z/iaR7\nJN3YxToT/LAIADI1DPSIuE/SBTXaH1NyPB0AUAD8UhQASoJAB4CSKH6gz80t919wwcphAMALih3o\nc3PS5OTy8NGjyTChDgAnKXagT01Ji4sr2xYXk3YAwArFDvQjR5prB4BVrNiBvnVrc+0AsIoVO9Cn\np6Xh4ZVtw8NJOwBghWIH+sSENDOzPLxlSzI8MdG/mgCgoIod6NLK8L7nHsIcADIUP9ABALkQ6ABQ\nEgQ6AJQEgQ4AJUGgA0BJDFagc4MLAMg0WIEOAMiU556iZ9u+2/aDth+w/a60/QO2j9nen3aXd79c\nAECWPPcUfVbS70XEf9h+saR9tu9KH7suIj7SvfIAAHnluafocUnH0/7/tf2QpLO6XRgAoDlNHUO3\nPabkhtF706Z32r7P9k22N2U8Z9L2vO35hYWFtooFAGTLHei2N0r6gqR3R8TTkm6Q9DJJ25XswX+0\n1vMiYiYixiNifHR0tAMlAwBqyRXottcpCfO5iLhNkiLiiYh4LiKel/SXki7qXpkAgEbynOViSTdK\neigiPlbRvrlitF+SdKDz5QEA8spzlssrJb1d0v2296dt75d0pe3tkkLSIUm/05UKAQC55DnL5WuS\nXOOhOztfDgCgVfxSFABKgkAHgJIg0AGgJAh0ACgJAh0ASoJAB4CSGKxA5wYXAJBpsAIdAJCJQAeA\nkiDQAaAkCHQAKInBCnTXuqQMAEAatEAHAGQi0AGgJAYr0DkPHQAyDVagAwAy5bkF3dm277b9oO0H\nbL8rbT/d9l22H0n/bup+uQCALHn20J+V9HsRcb6kiyX9ru3zJV0jaU9EnCdpTzoMAOiThoEeEccj\n4j/S/v+V9JCksyS9WdLudLTdkq7oVpEAgMaaOoZue0zSBZL2SjozIo6nDz0u6cyOVgYAaEruQLe9\nUdIXJL07Ip6ufCwiQlLNU1BsT9qetz2/sLDQVrEAgGy5At32OiVhPhcRt6XNT9jenD6+WdKJWs+N\niJmIGI+I8dHR0U7UDACoIc9ZLpZ0o6SHIuJjFQ99UdLOtH+npNs7X14VzkMHgExrc4zzSklvl3S/\n7f1p2/slfUjSLbavknRY0q90p0QAQB4NAz0iviYp66pYOzpbDgCgVYP1S1GutggAmQYr0AEAmYof\n6HNzy/0XXrhyGADwgmIH+tycNDm5PHz0aDJMqAPASYod6FNT0uLiyrbFxaQdALBCsQP9yJHm2gFg\nFSt2oG/d2lw7AKxixQ706WlpeHhl2/Bw0g4AWKHYgT4xIc3MLA9v2ZIMT0z0ryYAKKhiB7q0Mrzn\n5wlzAMhQ/EAHAORCoANASRDoAFASBDoAlASBDgAlQaADQEkQ6ABQEnnuKXqT7RO2D1S0fcD2Mdv7\n0+7y7pYJAGgkzx76ZyRdVqP9uojYnnZ3drYsAECzGgZ6RHxV0vd6UAsAoA3tHEN/p+370kMymzpW\nEQCgJa0G+g2SXiZpu6Tjkj6aNaLtSdvztucXFhZanB0AoJGWAj0inoiI5yLieUl/KemiOuPORMR4\nRIyPjo62WicAoIGWAt325orBX5J0IGvcjoroyWwAYBCtbTSC7c9Keo2kM2wflfTHkl5je7ukkHRI\n0u90sUYAQA4NAz0irqzRfGMXagEAtIFfigJASRDoAFASgxXodr8rAIDCGqxABwBkItABoCQGK9A5\nDx0AMg1WoAMAMhHoAFASBDoAlMRgBfpLXyqNjUlzc/2uBAAKp/iBXhneEdLhw9LkJKEOAFWKH+hT\nUye3LS7WbgeAVaz4gX74cO32I0d6WwcAFFyxA31uLvvn/lu39rYWACi4Ygf61FTtHxPZ0vR07+sB\ngAIrdqBnHW6JkCYmelsLABRcsQN9aCj7Mc5yAYAVGga67Ztsn7B9oKLtdNt32X4k/bupK9U991z2\nY5zlAgAr5NlD/4yky6rarpG0JyLOk7QnHe68bduyH+MsFwBYoWGgR8RXJX2vqvnNknan/bslXdHh\nuhLT05zlAgA5tXoM/cyIOJ72Py7pzA7Vs9LEhHT11Se3Dw9zlgsAVGn7S9GICEmZFyq3PWl73vb8\nwsJC8zO4/nrpF35heXjbNmlmhrNcAKBKq4H+hO3NkpT+PZE1YkTMRMR4RIyPjo62Nrfzz1/uP3SI\nMAeAGloN9C9K2pn275R0e2fKAQC0Ks9pi5+V9K+Sftz2UdtXSfqQpNfZfkTSa9Ph7sn6YhQA8IK1\njUaIiCszHtrR4VoAAG0o9i9Fl3BzaABoqPiBPjcnffrTK4cBACcpdqDPzSV3J3r66eW2t71Neu1r\n+1cTABRUsQN9aiq5O1G1PXukd7yj9/UAQIEVO9CzLp8rJT8uAgC8oNiBXu/yufWuxAgAq1CxA53Q\nBoDcih3oIyP9rgAABkaxA72eetdKB4BVqNiB/r3qy7BX4PK5ALBCsQM96yYWGzZwxUUAqFLsQJ+e\nrn2my49+xC9GAaBKsQNdqn2myw9/yE2iAaBKsQO9XmjX+9ERAKxCxQ70I0eyH6v3oyMAWIWKHehZ\nX4pK/OgIAKoUO9A5NREAcmsr0G0fsn2/7f225ztV1As4NREAcmt4C7ocfi4inuzAdE5W79REjqED\nwArFPuRS7yyXycne1dGsuTlpbExasyb5yznzAHqg3UAPSf9ke5/tzidsvbNcrr++47PriKW7LB0+\nnNwL9fDhZJhQR5Gw01FK7Qb6qyLiFZLeIOl3bb+6egTbk7bnbc8vLCw0N/V6P/0vqlp3WVpcXL0/\nhCI4ioedjtJqK9Aj4lj694Skv5N0UY1xZiJiPCLGR0dHm5vB9HQSBNW+//3i3oIu6wdP9T5tlBXB\nUUyDvtMxKDsJ/agzIlrqJG2Q9OKK/m9Iuqzecy688MJoWhIFtbuimZ2NsGvXum1bv6vrvW3bGq+L\n2dlk2E7+zs72p9ayqbdes16jUjJekbfJ7GzE8PDKmoeHi1VjRMfrlDQfeXI5z0g1nyidK+netHtA\n0lSj55Q60GdnI4aGatdpF+8F1wv1gsOOGBmJWL8+/4u+yEFTJI3CJOuNVopYt665bdJreXYSiqDD\ndXY90FvpuhLoef7JK8cZGUm6ToZCrX+gTrz5dCvAak23G/OqFxz1ulov+kHZM6ulE+u2mWk0CpM8\nr9dG26Rfb65ZOwl2d+fb7PJ2uM7VEei7dp38wlxakZVBVe/F24k9wkbBledduXpetZatEwFWa310\na6+sleDIetFnreORkZPXW7NB0044NXpuq29E1TshtbZP1rLmCZPZ2da3ya5dtecxMtJ4h6rd8B8Z\nyX4ddEsr25A99Mwlye6yDnFUh3uzex8RzW3ERocWll5wWZ8Mas2rW8fis/4hujGvpWVrNtBrzTfv\ntqzuGv3j1Vv3jcInz2ukmX/s2dnmtk/1Olmad9Y8h4aWa3v++da2S1aYZ627dj5Z7dq1/D8+NJQM\nZ62f9eu794mhlXAetGPorXQdD/ROddV7hPWOh1d+bF16Aa1Z0/w8161bDvhGb0z1aq2suXpPtfLF\nPzKStLWyXvLshdaa19JzKtddM+ulcl6tHr6pDrLq5Wm07tevz97rbPQaiagfflnrr51uad03egNs\n5jVX/ZrIO+7wcPZyNdpZaPa1mtW1ckix+tNRvek3ms6ppybjjY629SZDoDe70ZfkOUywYUNv6sr7\nj9DqoY08Xa1lrQzdkZH84bBjR+1pLT3/JS85+dCCFLFxY/vLsbR31Oq6qnyDqnUIpLKrfCOst252\n7erOtqtXW5G6eodnejH/WoeIWt0elW8Std6g3/Oe5rNvRQwS6Pm7HTuW59epPaVeviB79Q/QjW7X\nrojt25P+Vj7pNNNt29bbsGh1L5iud13lG327//tr1mS/ma5d25M9dCfj9sb4+HjMzzd5UUa7O8VU\nm52Vvv516YYbejM/YLVYu1Z69tl+V9F/IyPSk61dx9D2vogYbzResS/O1UtvexthDnQDYZ747ne7\n/gt3Ah0AeuWGG7p6CQACHQB66eqruzZpAh0AeumZZ7o2aQIdAEqCQAeAkiDQAaDXuvTFKIEOAL3W\npZuJEOgA0GtZdzZrU/EDfdu2flcAAJ01NNSVyRY/0Ken+10BAHTWc891ZbJtBbrty2w/bPug7Ws6\nVdQKExNdmSwA9E2Xjjy0HOi2hyR9UtIbJJ0v6Urb53eqMAAorS4deWhnD/0iSQcj4rGI+KGkz0l6\nc2fKAoAS69KRh3YC/SxJ36kYPpq2rWB70va87fmFhYXW5rRrV2vPA4BVpOtfikbETESMR8T46Oho\naxO5/npCHUA57NjRtUm3E+jHJJ1dMbwlbeuO669P7v0xOytt2ND56XfqRhqtno5kSxs3Jn83bJDW\nFP8EpLbZyRv1rl29u5FJu/NZty65UUEnplVLr7b72rWtPc+WTjml+ee0a8eOk18np5zSnSxoxshI\nkkmzs/m+6NyxQ/ryl7tXT57bGtXqJK2V9JikcyStl3SvpJ+s95yWbkFXNM3cbLaXOlFXrZs917vv\nYyfXQT/Wa5559mt7tzrfylsSLt0Cr2yv035Ov081qBe3oLN9uaSPSxqSdFNE1P3qtqVb0AHAKpf3\nFnQtfu5KRMSdku5sZxoAgM5YBQdqAWB1INABoCQIdAAoCQIdAEqirbNcmp6ZvSCp1QsBnyHpyQ6W\nMwhY5tWBZV4d2lnmbRHR8JeZPQ30dtiez3PaTpmwzKsDy7w69GKZOeQCACVBoANASQxSoM/0u4A+\nYJlXB5Z5dej6Mg/MMXQAQH2DtIcOAKhjIAK9J/cu7QHbZ9u+2/aDth+w/a60/XTbd9l+JP27KW23\n7U+ky32f7VdUTGtnOv4jtnf2a5nysj1k+x7bd6TD59jemy7b522vT9tPSYcPpo+PVUzj2rT9Yduv\n78+S5GP7NNu32v6W7YdsX1L27Wz7Penr+oDtz9p+Udm2s+2bbJ+wfaCirWPb1faFtu9Pn/MJu8lr\nD+e5JGM/OyVXcnxU0rlavkzv+f2uq8Vl2SzpFWn/iyX9p5L7sf6ppGvS9mskfTjtv1zSP0qypIsl\n7U3bT1dy6eLTJW1K+zf1e/kaLPt7Jf2NpDvS4VskvTXt/5SkXWn/OyR9Ku1/q6TPp/3np9v+FCWX\nbH5U0lC/l6vO8u6W9Ftp/3pJp5V5Oyu5W9m3JZ1asX1/vWzbWdKrJb1C0oGKto5tV0nfTMd1+tw3\nNFVfv1dQjhV4iaQvVQxfK+naftfVoWW7XdLrJD0saXPatlnSw2n/pyVdWTH+w+njV0r6dEX7ivGK\n1im5+ckeSZdKuiN9sT4paW31Npb0JUmXpP1r0/Fcvd0rxytaJ+klabi5qr2021nLt6Q8Pd1ud0h6\nfRm3s6SxqkDvyHZNH/tWRfuK8fJ0g3DIJde9SwdN+hHzAkl7JZ0ZEcfThx6XdGban7Xsg7ZOPi7p\nDyQ9nw6PSHoqIp5Nhyvrf2HZ0sf/Jx1/kJb5HEkLkm5ODzP9le0NKvF2johjkj4i6Yik40q22z6V\nezsv6dR2PSvtr27PbRACvXRsb5T0BUnvjoinKx+L5K25NKce2X6jpBMRsa/ftfTQWiUfy2+IiAsk\nfV/JR/EXlHA7b5L0ZiVvZi+VtEHSZX0tqg/6vV0HIdB7e+/SLrO9TkmYz0XEbWnzE7Y3p49vlnQi\nbc9a9kFaJ6+U9CbbhyR9Tslhlz+TdJrtpRusVNb/wrKlj79E0nc1WMt8VNLRiNibDt+qJODLvJ1f\nK+nbEbEQET+SdJuSbV/m7bykU9v1WNpf3Z7bIAT6v0s6L/22fL2SL1C+2OeaWpJ+Y32jpIci4mMV\nD31R0tI33TuVHFtfav+19NvyiyX9T/rR7kuSft72pnTP6OfTtsKJiGsjYktEjCnZdl+JiAlJd0t6\nSzpa9TIvrYu3pONH2v7W9OyIcySdp+QLpMKJiMclfcf2j6dNOyQ9qBJvZyWHWi62PZy+zpeWubTb\nuUJHtmv62NO2L07X4a9VTCuffn/BkPNLiMuVnBHyqKSpftfTxnK8SsnHsfsk7U+7y5UcO9wj6RFJ\nX5Z0ejq+JX0yXe77JY1XTOs3JR1Mu9/o97LlXP7XaPksl3OV/KMelPS3kk5J21+UDh9MHz+34vlT\n6bp4WE1++9+HZd0uaT7d1n+v5GyGUm9nSR+U9C1JByT9tZIzVUq1nSV9Vsl3BD9S8knsqk5uV0nj\n6fp7VNJfqOqL9UYdvxQFgJIYhEMuAIAcCHQAKAkCHQBKgkAHgJIg0AGgJAh0lJrtZ/pdA9ArBDoA\nlASBjlXH9pjtr6TXqN5je2va/svptbzvtf3VtO0nbX/T9v50/PP6Wz2QjR8WodRsPxMRG6va/kHS\nrRGx2/ZvSnpTRFxh+35Jl0XEMdunRcRTtv9c0r9FxFx66YmhiPi/PiwK0BB76FiNLlFysw0p+Yn6\nq9L+r0v6jO3fVnJjFUn6V0nvt/0+SdsIcxQZgQ6kIuJqSX+o5Ep4+2yPRMTfSHqTpP+TdKftS/tZ\nI1APgY7V6BtKrvwoSROS/kWSbL8sIvZGxB8puUHF2bbPlfRYRHxCyZXvfrofBQN5cAwdpWb7eUn/\nVdH0MSXXo79Z0hlKgvs3IuKI7duUXK7VSq6e925J75P0diVX13tc0q9GxPd6twRAfgQ6AJQEh1wA\noCQIdAAoCQIdAEqCQAeAkiDQAaAkCHQAKAkCHQBKgkAHgJL4f2IuJs7n0aqjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9b1daa3d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoches=10000\n",
    "for i in range(epoches):\n",
    "    loss = 0\n",
    "    optimizer.zero_grad()#清空上一步的梯度\n",
    "    idx = np.random.randint(x.shape[0], size=batch_size)\n",
    "    batch_cpu = Variable(torch.from_numpy(x[idx])).float()\n",
    "    #batch = batch_cpu.cuda()#很重要\n",
    "    batch = batch_cpu.to(device)\n",
    "    \n",
    "    target_cpu = Variable(torch.from_numpy(y[idx])).float()\n",
    "    #target = target_cpu.cuda()#很重要\n",
    "    target = target_cpu.to(device)\n",
    "    output = model.forward(batch).squeeze(1)\n",
    "    loss += criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (i +1)%10 == 0:\n",
    "        print('Loss at epoch[%s]: %.3f' % (i, loss.data[0]))\n",
    "    losses.append(loss.data[0])\n",
    "\n",
    "plt.plot(losses, '-or')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.xlabel(\"Loss\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
