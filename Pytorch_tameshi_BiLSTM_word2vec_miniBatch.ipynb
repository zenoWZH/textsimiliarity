{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# 設定ファイル\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import collections\n",
    "import MeCab\n",
    "import numpy as np\n",
    "from gensim.models import word2vec\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "w2v_model = word2vec.Word2Vec.load(\"/ssd/local-politics/word2vec/word2vec.model\")\n",
    "features = [line.strip().split(\"\\t\")[-1] for line in open(\"features.txt\", \"r\")]\n",
    "\n",
    "\n",
    "# 単語に切り分ける\n",
    "# 引数: 文(utf-8)\n",
    "# 返値: 単語の配列(utf-8)\n",
    "def get_tango(sen):\n",
    "    word_list = []\n",
    "    t = MeCab.Tagger(\"aa\")\n",
    "    for word_line in t.parse(sen).split(\"\\n\"):\n",
    "        if word_line.strip() == \"EOS\":\n",
    "            break\n",
    "        (word, temp) = word_line.split(\"\\t\")\n",
    "        temps = temp.split(',')\n",
    "        if \"記号\" == temps[0]:\n",
    "            continue\n",
    "        if \"数\" == temps[1]:\n",
    "            continue\n",
    "        word_list.append(word)\n",
    "    return word_list\n",
    "    \n",
    "    \n",
    "# 文を受けとり、素性データを用いて、入力データを生成\n",
    "# 引数: 文(utf-8)\n",
    "# 返値: NNの入力ベクトル\n",
    "def sentence2inputs(sentence):\n",
    "    global features, w2v_model\n",
    "    word_list = get_tango(sentence)\n",
    "    return [w2v_model[word] for word in word_list if word in features]\n",
    "\n",
    "\n",
    "# ファイルパスを受け取り、そこから文を読み込みSVM用に加工し返す\n",
    "# 引数: ファイルパス, 素性フラグ\n",
    "# 返値: 素性配列とタグ配列のタプル\n",
    "def getSvmData(fname, flag):\n",
    "    # 学習データ読み込み\n",
    "    lines = [line.strip() for line in open(fname, \"r\")]\n",
    "    sentences = [line.split('\\t')[-1] for line in lines]\n",
    "    tags = [int(line.split(\"\\t\")[0]) for line in lines]\n",
    "    #tags = [(i, 0 if i == 1 else 1) for i in temp]\n",
    "    svmdata = [sentence2inputs(sentence) for sentence in sentences]\n",
    "    while svmdata.count([]) != 0:\n",
    "        idx = svmdata.index([])\n",
    "        del tags[idx]\n",
    "        del svmdata[idx]\n",
    "    return (svmdata, tags)\n",
    "\n",
    "(train, trainTag) = getSvmData('../data/testData.txt', False)\n",
    "(test, testTag) = getSvmData('../data/additionalTestData.txt', False)\n",
    "\n",
    "\n",
    "# DatasetのMyクラス: 入力はword2vecを想定\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, tags):\n",
    "        super(MyDataset, self).__init__()\n",
    "        assert len(data) == len(tags)\n",
    "        # npに変換し、0埋めを行う\n",
    "        max_length = max([len(d) for d in data])\n",
    "        self.data = np.zeros((len(tags), max_length, len(data[0][0])))\n",
    "        for i, d1 in enumerate(data):\n",
    "            for l, d2 in enumerate(d1):\n",
    "                self.data[i][l] = d2\n",
    "        self.tags = tags\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.tags)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.tags[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (lstm): LSTM(10, 100, bidirectional=True)\n",
      "  (fc0): Linear(in_features=200, out_features=40, bias=True)\n",
      "  (fc1): Linear(in_features=40, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_dim, bidirectional=False, batch_size=1):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, bidirectional=bidirectional)\n",
    "        self.fc0 = nn.Linear(hidden_dim * 2, 40)\n",
    "        self.fc1 = nn.Linear(40, 2)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def forward(self, inputs, lengths):\n",
    "        # 行と列を入れ替える\n",
    "        inputs = inputs.transpose(0, 1)\n",
    "        pack = torch.nn.utils.rnn.pack_padded_sequence(inputs, lengths)\n",
    "        lstm_out, self.hidden = self.lstm(pack, self.hidden)\n",
    "        y = self.fc0(torch.cat([self.hidden[0][-1], self.hidden[0][-2]], 1))\n",
    "        y = self.fc1(F.tanh(y))\n",
    "        tag_scores = F.log_softmax(y)\n",
    "        return tag_scores\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        # (hidden_state, cell_state)のタプルになる\n",
    "        num = 2 if self.bidirectional else 1\n",
    "        return (Variable(torch.zeros(num, self.batch_size, self.hidden_dim)).cuda(),\n",
    "                Variable(torch.zeros(num, self.batch_size, self.hidden_dim)).cuda())\n",
    "\n",
    "\n",
    "#net = LSTM(len(train[0][0]), 100, True)\n",
    "net = LSTM(10,100,True)\n",
    "net.cuda()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch[0]: Loss: 1.8081\n",
      "===> Epoch[1]: Loss: 1.6085\n",
      "===> Epoch[2]: Loss: 1.4709\n",
      "===> Epoch[3]: Loss: 1.4013\n",
      "===> Epoch[4]: Loss: 1.3998\n",
      "===> Epoch[5]: Loss: 1.3114\n",
      "===> Epoch[6]: Loss: 1.3363\n",
      "===> Epoch[7]: Loss: 1.2934\n",
      "===> Epoch[8]: Loss: 1.2239\n",
      "===> Epoch[9]: Loss: 1.1971\n",
      "===> Epoch[10]: Loss: 1.1496\n",
      "===> Epoch[11]: Loss: 1.2017\n",
      "===> Epoch[12]: Loss: 1.1313\n",
      "===> Epoch[13]: Loss: 1.0645\n",
      "===> Epoch[14]: Loss: 1.0932\n",
      "===> Epoch[15]: Loss: 1.0192\n",
      "===> Epoch[16]: Loss: 0.9817\n",
      "===> Epoch[17]: Loss: 0.9589\n",
      "===> Epoch[18]: Loss: 0.8938\n",
      "===> Epoch[19]: Loss: 0.8337\n",
      "===> Epoch[20]: Loss: 0.9431\n",
      "===> Epoch[21]: Loss: 0.8259\n",
      "===> Epoch[22]: Loss: 0.8045\n",
      "===> Epoch[23]: Loss: 0.7304\n",
      "===> Epoch[24]: Loss: 0.7186\n",
      "===> Epoch[25]: Loss: 0.6778\n",
      "===> Epoch[26]: Loss: 0.6763\n",
      "===> Epoch[27]: Loss: 0.5879\n",
      "===> Epoch[28]: Loss: 0.6312\n",
      "===> Epoch[29]: Loss: 0.5570\n",
      "===> Epoch[30]: Loss: 0.5553\n",
      "===> Epoch[31]: Loss: 0.5821\n",
      "===> Epoch[32]: Loss: 0.5140\n",
      "===> Epoch[33]: Loss: 0.4342\n",
      "===> Epoch[34]: Loss: 0.4674\n",
      "===> Epoch[35]: Loss: 0.4180\n",
      "===> Epoch[36]: Loss: 0.4049\n",
      "===> Epoch[37]: Loss: 0.4122\n",
      "===> Epoch[38]: Loss: 0.3829\n",
      "===> Epoch[39]: Loss: 0.3273\n",
      "===> Epoch[40]: Loss: 0.3145\n",
      "===> Epoch[41]: Loss: 0.3085\n",
      "===> Epoch[42]: Loss: 0.3002\n",
      "===> Epoch[43]: Loss: 0.3017\n",
      "===> Epoch[44]: Loss: 0.2664\n",
      "===> Epoch[45]: Loss: 0.2829\n",
      "===> Epoch[46]: Loss: 0.2039\n",
      "===> Epoch[47]: Loss: 0.2234\n",
      "===> Epoch[48]: Loss: 0.2235\n",
      "===> Epoch[49]: Loss: 0.2387\n",
      "===> Epoch[50]: Loss: 0.1844\n",
      "===> Epoch[51]: Loss: 0.1953\n",
      "===> Epoch[52]: Loss: 0.1682\n",
      "===> Epoch[53]: Loss: 0.1477\n",
      "===> Epoch[54]: Loss: 0.1466\n",
      "===> Epoch[55]: Loss: 0.1369\n",
      "===> Epoch[56]: Loss: 0.1387\n",
      "===> Epoch[57]: Loss: 0.1344\n",
      "===> Epoch[58]: Loss: 0.1152\n",
      "===> Epoch[59]: Loss: 0.1053\n",
      "===> Epoch[60]: Loss: 0.1008\n",
      "===> Epoch[61]: Loss: 0.1047\n",
      "===> Epoch[62]: Loss: 0.0988\n",
      "===> Epoch[63]: Loss: 0.1834\n",
      "===> Epoch[64]: Loss: 0.0926\n",
      "===> Epoch[65]: Loss: 0.0806\n",
      "===> Epoch[66]: Loss: 0.0832\n",
      "===> Epoch[67]: Loss: 0.0815\n",
      "===> Epoch[68]: Loss: 0.0799\n",
      "===> Epoch[69]: Loss: 0.0661\n",
      "===> Epoch[70]: Loss: 0.0685\n",
      "===> Epoch[71]: Loss: 0.0719\n",
      "===> Epoch[72]: Loss: 0.0619\n",
      "===> Epoch[73]: Loss: 0.0650\n",
      "===> Epoch[74]: Loss: 0.0574\n",
      "===> Epoch[75]: Loss: 0.0500\n",
      "===> Epoch[76]: Loss: 0.0476\n",
      "===> Epoch[77]: Loss: 0.0451\n",
      "===> Epoch[78]: Loss: 0.0465\n",
      "===> Epoch[79]: Loss: 0.0490\n",
      "===> Epoch[80]: Loss: 0.0439\n",
      "===> Epoch[81]: Loss: 0.0551\n",
      "===> Epoch[82]: Loss: 0.0453\n",
      "===> Epoch[83]: Loss: 0.0405\n",
      "===> Epoch[84]: Loss: 0.0379\n",
      "===> Epoch[85]: Loss: 0.0362\n",
      "===> Epoch[86]: Loss: 0.0360\n",
      "===> Epoch[87]: Loss: 0.0330\n",
      "===> Epoch[88]: Loss: 0.0355\n",
      "===> Epoch[89]: Loss: 0.0312\n",
      "===> Epoch[90]: Loss: 0.0299\n",
      "===> Epoch[91]: Loss: 0.0284\n",
      "===> Epoch[92]: Loss: 0.0287\n",
      "===> Epoch[93]: Loss: 0.0270\n",
      "===> Epoch[94]: Loss: 0.0261\n",
      "===> Epoch[95]: Loss: 0.0260\n",
      "===> Epoch[96]: Loss: 0.0277\n",
      "===> Epoch[97]: Loss: 0.0256\n",
      "===> Epoch[98]: Loss: 0.0252\n",
      "===> Epoch[99]: Loss: 0.0234\n",
      "===> Epoch[100]: Loss: 0.0231\n",
      "===> Epoch[101]: Loss: 0.0207\n",
      "===> Epoch[102]: Loss: 0.0196\n",
      "===> Epoch[103]: Loss: 0.0217\n",
      "===> Epoch[104]: Loss: 0.0208\n",
      "===> Epoch[105]: Loss: 0.0199\n",
      "===> Epoch[106]: Loss: 0.0209\n",
      "===> Epoch[107]: Loss: 0.0182\n",
      "===> Epoch[108]: Loss: 0.0190\n",
      "===> Epoch[109]: Loss: 0.0186\n",
      "===> Epoch[110]: Loss: 0.0176\n",
      "===> Epoch[111]: Loss: 0.0184\n",
      "===> Epoch[112]: Loss: 0.0162\n",
      "===> Epoch[113]: Loss: 0.0186\n",
      "===> Epoch[114]: Loss: 0.0176\n",
      "===> Epoch[115]: Loss: 0.0170\n",
      "===> Epoch[116]: Loss: 0.0169\n",
      "===> Epoch[117]: Loss: 0.0159\n",
      "===> Epoch[118]: Loss: 0.0165\n",
      "===> Epoch[119]: Loss: 0.0155\n",
      "===> Epoch[120]: Loss: 0.0153\n",
      "===> Epoch[121]: Loss: 0.0144\n",
      "===> Epoch[122]: Loss: 0.0144\n",
      "===> Epoch[123]: Loss: 0.0151\n",
      "===> Epoch[124]: Loss: 0.0149\n",
      "===> Epoch[125]: Loss: 0.0136\n",
      "===> Epoch[126]: Loss: 0.0132\n",
      "===> Epoch[127]: Loss: 0.0127\n",
      "===> Epoch[128]: Loss: 0.0122\n",
      "===> Epoch[129]: Loss: 0.0124\n",
      "===> Epoch[130]: Loss: 0.0120\n",
      "===> Epoch[131]: Loss: 0.0122\n",
      "===> Epoch[132]: Loss: 0.0126\n",
      "===> Epoch[133]: Loss: 0.0132\n",
      "===> Epoch[134]: Loss: 0.0112\n",
      "===> Epoch[135]: Loss: 0.0118\n",
      "===> Epoch[136]: Loss: 0.0107\n",
      "===> Epoch[137]: Loss: 0.0127\n",
      "===> Epoch[138]: Loss: 0.0110\n",
      "===> Epoch[139]: Loss: 0.0106\n",
      "===> Epoch[140]: Loss: 0.0122\n",
      "===> Epoch[141]: Loss: 0.0107\n",
      "===> Epoch[142]: Loss: 0.0099\n",
      "===> Epoch[143]: Loss: 0.0116\n",
      "===> Epoch[144]: Loss: 0.0106\n",
      "===> Epoch[145]: Loss: 0.0107\n",
      "===> Epoch[146]: Loss: 0.0098\n",
      "===> Epoch[147]: Loss: 0.0102\n",
      "===> Epoch[148]: Loss: 0.0092\n",
      "===> Epoch[149]: Loss: 0.0092\n",
      "===> Epoch[150]: Loss: 0.0098\n",
      "===> Epoch[151]: Loss: 0.0091\n",
      "===> Epoch[152]: Loss: 0.0092\n",
      "===> Epoch[153]: Loss: 0.0093\n",
      "===> Epoch[154]: Loss: 0.0090\n",
      "===> Epoch[155]: Loss: 0.0088\n",
      "===> Epoch[156]: Loss: 0.0097\n",
      "===> Epoch[157]: Loss: 0.0084\n",
      "===> Epoch[158]: Loss: 0.0093\n",
      "===> Epoch[159]: Loss: 0.0090\n",
      "===> Epoch[160]: Loss: 0.0086\n",
      "===> Epoch[161]: Loss: 0.0083\n",
      "===> Epoch[162]: Loss: 0.0074\n",
      "===> Epoch[163]: Loss: 0.0087\n",
      "===> Epoch[164]: Loss: 0.0080\n",
      "===> Epoch[165]: Loss: 0.0079\n",
      "===> Epoch[166]: Loss: 0.0075\n",
      "===> Epoch[167]: Loss: 0.0075\n",
      "===> Epoch[168]: Loss: 0.0069\n",
      "===> Epoch[169]: Loss: 0.0076\n",
      "===> Epoch[170]: Loss: 0.0075\n",
      "===> Epoch[171]: Loss: 0.0070\n",
      "===> Epoch[172]: Loss: 0.0066\n",
      "===> Epoch[173]: Loss: 0.0076\n",
      "===> Epoch[174]: Loss: 0.0071\n",
      "===> Epoch[175]: Loss: 0.0068\n",
      "===> Epoch[176]: Loss: 0.0077\n",
      "===> Epoch[177]: Loss: 0.0065\n",
      "===> Epoch[178]: Loss: 0.0065\n",
      "===> Epoch[179]: Loss: 0.0068\n",
      "===> Epoch[180]: Loss: 0.0073\n",
      "===> Epoch[181]: Loss: 0.0064\n",
      "===> Epoch[182]: Loss: 0.0065\n",
      "===> Epoch[183]: Loss: 0.0061\n",
      "===> Epoch[184]: Loss: 0.0058\n",
      "===> Epoch[185]: Loss: 0.0065\n",
      "===> Epoch[186]: Loss: 0.0063\n",
      "===> Epoch[187]: Loss: 0.0066\n",
      "===> Epoch[188]: Loss: 0.0061\n",
      "===> Epoch[189]: Loss: 0.0059\n",
      "===> Epoch[190]: Loss: 0.0061\n",
      "===> Epoch[191]: Loss: 0.0061\n",
      "===> Epoch[192]: Loss: 0.0059\n",
      "===> Epoch[193]: Loss: 0.0053\n",
      "===> Epoch[194]: Loss: 0.0062\n",
      "===> Epoch[195]: Loss: 0.0059\n",
      "===> Epoch[196]: Loss: 0.0055\n",
      "===> Epoch[197]: Loss: 0.0066\n",
      "===> Epoch[198]: Loss: 0.0052\n",
      "===> Epoch[199]: Loss: 0.0053\n"
     ]
    }
   ],
   "source": [
    "dataset = MyDataset(train, trainTag)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=256, shuffle=True, num_workers=8)\n",
    "\n",
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1)\n",
    "criterion = nn.NLLLoss()\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(200):  # データセットに渡り複数回ループ\n",
    "\n",
    "    #データ全てのトータルロス\n",
    "    epoch_loss = 0.0 \n",
    "    \n",
    "    for i, data in enumerate(dataloader):\n",
    "        inputs, labels = data\n",
    "\n",
    "        temp = []\n",
    "        for d1 in inputs:\n",
    "            temp.append(sum([1 if d2[0] != 0 else 0 for d2 in d1]))\n",
    "        lens = torch.LongTensor(temp)\n",
    "        # 大きい順に並び替える\n",
    "        lens, idx = lens.sort(0, descending=True)\n",
    "        inputs = inputs[idx]\n",
    "        labels = labels[idx]\n",
    "\n",
    "        inputs, labels = Variable(inputs.float().cuda()), Variable(labels.cuda())\n",
    "        \n",
    "        net.batch_size = len(labels)\n",
    "        optimizer.zero_grad()\n",
    "        net.hidden = net.init_hidden()\n",
    "        \n",
    "        output = net(inputs, lens.tolist())\n",
    "        \n",
    "        loss = criterion(output, labels)\n",
    "        epoch_loss += loss.data[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # ロスの表示\n",
    "    print(\"===> Epoch[{}]: Loss: {:.4f}\".format(epoch, epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 57.0 / 67.0 = 0.8507462686567164\n",
      "Recall: 57.0 / 78.0 = 0.7307692307692307\n",
      "F-measure: 0.786206896551724\n"
     ]
    }
   ],
   "source": [
    "dataset = MyDataset(test, testTag)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=256, shuffle=True, num_workers=8)\n",
    "\n",
    "result = []\n",
    "labelList = []\n",
    "for i, data in enumerate(dataloader):\n",
    "    inputs, labels = data\n",
    "\n",
    "    temp = []\n",
    "    for d1 in inputs:\n",
    "        temp.append(sum([1 if d2[0] != 0 else 0 for d2 in d1]))\n",
    "    lens = torch.LongTensor(temp)\n",
    "    # 大きい順に並び替える\n",
    "    lens, idx = lens.sort(0, descending=True)\n",
    "    inputs = inputs[idx]\n",
    "    labels = labels[idx]\n",
    "    \n",
    "    inputs, labels = Variable(inputs.float().cuda()), Variable(labels.cuda())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    net.batch_size = len(labels)\n",
    "    net.hidden = net.init_hidden()\n",
    "    output = net(inputs, lens.tolist())\n",
    "    \n",
    "    # labelsを保存\n",
    "    labelList.extend([l for l in labels])\n",
    "    result.extend(output)\n",
    "\n",
    "result = [torch.max(temp, 0)[1].data[0] for temp in result]\n",
    "\n",
    "select = float(sum(result))\n",
    "zenbu = float(sum(testTag))\n",
    "seikai = 0.0\n",
    "for a, b in zip(result, labelList):\n",
    "    b = b.data[0]\n",
    "    if (a == 1) and (b == 1) and (a == b):\n",
    "        seikai += 1.0\n",
    "pre = seikai / select\n",
    "recall = seikai / zenbu\n",
    "f1 = 2 * pre * recall / (pre + recall)\n",
    "print(\"Precision: {} / {} = {}\".format(seikai, select, pre))\n",
    "print(\"Recall: {} / {} = {}\".format(seikai, zenbu, recall))\n",
    "print(\"F-measure: {}\".format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
